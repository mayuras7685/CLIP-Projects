# CLIP Image and Text Classification ğŸ“¸ğŸ“

This repository contains a simple Python script that utilizes the CLIP (Contrastive Language-Image Pretraining) model for image and text classification. CLIP is a neural network-based model that can be used for various tasks involving both images and text.

## Requirements ğŸ› ï¸

- Python 3
- PyTorch
- CLIP library
- PIL (Python Imaging Library)

## Installation ğŸš€

1. Install required dependencies:

   ```bash
   pip install torch clip PIL
   ```
## Usage ğŸš€

1. Run the script:

   ```bash
   python app.py
   ```
2. View Results:
   The script will print the label probabilities for the given example image and text.

## Customization ğŸ¨
You can customize the input image and text in the app.py script. Additionally, you may explore other CLIP model architectures based on your requirements.

## Acknowledgments ğŸ‘
This project uses the CLIP model developed by OpenAI. Check the [CLIP](https://github.com/openai/CLIP) repository for more details.
