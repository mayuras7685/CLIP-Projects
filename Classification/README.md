# CLIP Image and Text Classification 📸📝

This repository contains a simple Python script that utilizes the CLIP (Contrastive Language-Image Pretraining) model for image and text classification. CLIP is a neural network-based model that can be used for various tasks involving both images and text.

## Requirements 🛠️

- Python 3
- PyTorch
- CLIP library
- PIL (Python Imaging Library)

## Installation 🚀

1. Install required dependencies:

   ```bash
   pip install torch clip PIL
   ```
## Usage 🚀

1. Run the script:

   ```bash
   python app.py
   ```
2. View Results:
   The script will print the label probabilities for the given example image and text.

## Customization 🎨
You can customize the input image and text in the app.py script. Additionally, you may explore other CLIP model architectures based on your requirements.

## Acknowledgments 👏
This project uses the CLIP model developed by OpenAI. Check the [CLIP](https://github.com/openai/CLIP) repository for more details.
